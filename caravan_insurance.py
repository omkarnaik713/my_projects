# -*- coding: utf-8 -*-
"""Caravan Insurance

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gdjRpofxQQHBVBk7zDOtb27rwKinLFDo
"""

import pandas as pd
dataset = pd.read_csv('caravan-insurance-challenge.csv')
dataset.head()

dataset.info()

"""Apart from the first column 'ORIGIN' all the columns are of type int."""

dataset.isnull().sum()

"""There are no null values present in the dataset"""

dataset['ORIGIN'].value_counts()

dataset['CARAVAN'].value_counts()

import seaborn as sns
sns.countplot(dataset, x = 'CARAVAN')

"""The dataset is unbalanced

Methods to balance the data:
- Undersampling : Reducing the number of '0' to match the '1'
- Oversampling : Increasing the datapoints with similar data not duplicatate to match the number of '1' to '0'

**1. Using Under Sampling**
"""

from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler()
x = dataset.drop('CARAVAN',axis = 1)
y = dataset['CARAVAN']

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
label = le.fit_transform(x['ORIGIN'])
x['Origin'] = label
x.drop('ORIGIN',inplace = True , axis = 1)

x_us,y_us = rus.fit_resample(x,y)

x_us.count()

y_us.count()

y_us

"""Now the values of the both the classes is same 1172 hence the data is now balanced.
The drawback of undersampling is that we may lose important data while reducing the number of data points

---

**2. Over Sampling**

The methods we will be using for over sampling are
- RandomOverSampler: As the name suggests it randomly increases the number of data points of the lesser class
- SMOTE
"""

from imblearn.over_sampling import RandomOverSampler,SMOTE

"""**Using RandomOverSampler**"""

ros = RandomOverSampler()
x_ros,y_ros = ros.fit_resample(x,y)

x_ros.count()

print(y_ros)

"""**Using SMOTE**

SMOTE stands for Synthetic Minority Oversampling Technique where we use the existing data points of the minority class and create new similar but not duplicate datapoints
"""

x_smote,y_smote = SMOTE().fit_resample(x,y)

"""SMOTE can only be used to data points which are in int or numberical values it does not work for data types like string or character since we have already coverted the 'ORIGIN' column into categorical data there is no error"""

x.head()

x_smote.count()

y_smote.count()

"""We will be using Classification models to predict if they will take insurance or not where 0 - Not Taken 1 - Taken"""

x_us.describe()

x_ros.describe()

x_smote.describe()

import seaborn as sns
sns.heatmap(x_smote)

"""Doubt : How to present a heatmap with more readability and improve the above heatmap"""

# importing models

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC

"""Q - How do I know if the data is linearly seperable or not?"""

ls = LogisticRegression()
rfc = RandomForestClassifier()
svc = SVC()

"""Splitting the dataset using train_test_split

1. UnderSmapled data
"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x_us,y_us, test_size = 0.2)

ls.fit(x_train,y_train)

from sklearn.metrics import accuracy_score

accuracy_score(y_test,ls.predict(x_test))

rfc.fit(x_train,y_train)
print(accuracy_score(y_test,rfc.predict(x_test)))
svc.fit(x_train,y_train)
print(accuracy_score(y_test,svc.predict(x_test)))

"""2. Oversampled Data(RandomOverSampler)"""

x_train,x_test,y_train,y_test = train_test_split(x_ros,y_ros,test_size = 0.2)

ls_ros = LogisticRegression()
rfc_ros = RandomForestClassifier()
svc_ros = SVC()

ls_ros.fit(x_train,y_train)

rfc_ros.fit(x_train,y_train)
svc_ros.fit(x_train,y_train)

print(accuracy_score(y_test,ls_ros.predict(x_test)))

print(accuracy_score(y_test,rfc_ros.predict(x_test)))
print(accuracy_score(y_test,svc_ros.predict(x_test)))

"""3. OverSampling(SMOTE)"""

x_train,x_test,y_train,y_test = train_test_split(x_smote,y_smote, test_size = 0.2)
ls_s = LogisticRegression()
rfc_s = RandomForestClassifier()
svc_s = SVC()

ls_s.fit(x_train,y_train)
rfc_s.fit(x_train,y_train)
svc_s.fit(x_train,y_train)

print(accuracy_score(y_test,ls_s.predict(x_test)))
print(accuracy_score(y_test,rfc_s.predict(x_test)))
print(accuracy_score(y_test,svc_s.predict(x_test)))

"""---

**Inference:** From the accuracies of all the models on different types of sampling techniques we can infer that oversampling using SMOTE gives the best results when compared to UnderSampling and Oversampling usinf RandomOverSampler for the given Dataset

---
Next step now is to improve the accuracy of the model by using the appropriate hyperparameters

For this we will be using GRID SEARCH to understand how the model performs for different hyperparameters and choose the best model parameters.
"""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'penalty' : ['l2'],
    'C' : [0.1, 0.2,0.5,0.7],
    'fit_intercept' : [True,False],
    'intercept_scaling' : [0.2,0.4,0.6,0.8],
    'solver' : ['lbfgs','liblinear','newton-cg','newton-cholesky','sag','saga'],
    'max_iter' : [50,125,150,175],
}

ls_gs = GridSearchCV(LogisticRegression(),param_grid, n_jobs=-1, scoring ='roc_auc', verbose=  2, cv=5)
ls_gs.fit(x_train,y_train)
print('Best Prarameters : ', ls_gs.best_params_)

ls_2 = LogisticRegression(penalty='l2', C=0.7,fit_intercept=True, intercept_scaling = 0.2, max_iter = 50, solver = 'newton-cholesky')
ls_2.fit(x_train,y_train)
print(accuracy_score(y_test,ls_2.predict(x_test)))

"""After using the best hyperparameter the accuracy we get by using logistic Regression is 90%

---

Finding the best parameters for RandomForestClassifier
"""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'n_estimators' : [50,75,100,125],
    'criterion' : ['gini','entropy','log_loss'],
    'max_features' : ['sqrt','log2'],
    'n_jobs' : [-1],
    'class_weight' : ['balanced','balanced_subsample'],
}
rfc_gs = GridSearchCV(RandomForestClassifier(),param_grid,n_jobs=-1, scoring='roc_auc')
rfc_gs.fit(x_train,y_train)
print('Best Parameters : ', rfc_gs.best_params_)

rfc_bp = RandomForestClassifier(n_estimators=125,criterion='log_loss',max_features='log2',n_jobs=-1,class_weight='balanced_subsample')
rfc_bp.fit(x_train,y_train)
print(accuracy_score(y_test,rfc_bp.predict(x_test)))

"""The change in the accuracy of the RandomForestClassifier is very minute for this dataset."""

from sklearn.model_selection import GridSearchCV
param_grid = {
    'C' : [0.5,1,1.5,2],
    'kernel' : ['linear','poly','rbf','sigmoid'],
    'degree' : [2,3,4,5],
    'gamma' : ['scale','auto'],
    'class_weight' : ['dict','balanced'],
    'max_iter' : [50,100,150,-1],
    'decision_function_shape' : ['ovo','ovr'],
}

svc_gs= GridSearchCV(SVC(),param_grid,n_jobs=-1,scoring='roc_auc',verbose=2,cv=5)
svc_gs.fit(x_train,y_train)
print('Best Parameters : ', svc_gs.best_params_)

dataset.corr()

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

f,ax = plt.subplots(figsize=(10,8))
corr = x_smote.corr()
sns.heatmap(corr,mask=np.zeros_like(corr,dtype = np.bool),
            cmap= sns.diverging_palette(220,10,as_cmap=True),square= True , ax=ax)

len(dataset.axes[1])

column_list=[]
for col in x_train.columns:
  column_list.append(col)

print(column_list)

from scipy.stats import pearsonr
list3 = []
for i in range(86):
  list1 = x_train[column_list[i]]

  for j in range(i+1,86):
    list2 = x_train[column_list[j]]
    corr,_ = pearsonr(list1,list2)
    if corr < 0.0003 and corr > -0.0003 :
      print(column_list[i],column_list[j],corr)
      if column_list[i] not in list3 :
        list3.append(column_list[i])
      if column_list[j] not in list3 :
        list3.append(column_list[j])

"""By taking a threshold value for the correlation between the features as 0.005 we have got a list of features along with their correlations. Using this we will train the new model to check if there is any improvement or atleast or par with the previous model trained"""

len(list3)

for i in range(1,86):
  list1 = dataset['CARAVAN']

  list2= dataset[column_list[i]]
  corr,_ = pearsonr(list1,list2)
  print('Caravan ',column_list[i],corr )

for i in range (1,86) :
  rfc_1 = RandomForestClassifier(n_estimators=125,criterion='log_loss',max_features='log2',n_jobs=-1,class_weight='balanced_subsample')
  rfc_1.fit(x_train.drop(column_list[i],axis = 1),y_train)
  print(accuracy_score(y_test,rfc_1.predict(x_test.drop(column_list[i],axis = 1))))

"""Inference : In this code the problem is that the model should not have any prior knowledge of the previous iteration since that is not the case we are getting the accuarcy of each iteration similar.

The other assumption can be that each and every feature is contributing smimlarly.

To prove this we will have to use a new model for every iteration.

"""

for i in range(1,86) :
  print(accuracy_score(y_test,RandomForestClassifier().fit(x_train.drop(column_list[i],axis = 1),y_train).predict(x_test.drop(column_list[i],axis = 1))))

df = x_train.copy()
for i in range(len(column_list)) :
  if column_list[i] not in list3 :
    df.drop(column_list[i],axis = 1, inplace=True)

df.head()

"""Here we have reduced the data set from 86 columns to 14 columns.

Now let us check what affect it will have on the performance of the mode.
"""

df_test = x_test.copy()
for i in range(len(column_list)):
  if column_list[i] not in list3 :
    df_test.drop(column_list[i],axis = 1 , inplace=True)

rfc_fs = RandomForestClassifier(n_estimators=125,criterion='log_loss',max_features='log2',n_jobs=-1,class_weight='balanced_subsample')
rfc_fs.fit(df,y_train)
print(accuracy_score(y_test,rfc_fs.predict(df_test)))

"""As we can see the accuracy has droped to 89.87% so we need to change the treshold value

Now that we changed the threshold value to 0.0003 we got 36 columns now let us check how it affects the accuracy of the model.

The accuracy only dropped by 2% that is from 96% to 94%.

This in turn reduces the computational power required to process 86 columns by reducing it to 36.

Let us further try to reduce the number of columns to find the least number of columns which can give a similar accuracy.
"""

f,ax = plt.subplots(figsize=(10,8))
corr = df.corr()
sns.heatmap(corr,mask = np.zeros_like(corr,dtype= np.bool),
            cmap=sns.diverging_palette(220,10,as_cmap=True),square = True,ax=ax)

"""After reducing the columns to 36 the heatmap is more clear and understandable."""

column_list2 =['MKOOPKLA','MINKGEM','MINKM30','MSKA','MFWEKIND','MOPLLAAG','PMOTSCO','PVRAAUT','PAANHANG','PWERKT','PPLEZIER']
for i in range(len(column_list2)) :
  df.drop(column_list2[i],axis = 1 ,inplace = True)
  df_test.drop(column_list2[i],axis=1, inplace = True)

rfc_3 = RandomForestClassifier(n_estimators= 125,criterion='log_loss',max_features='log2',n_jobs=-1,class_weight='balanced_subsample')
rfc_3.fit(df,y_train)
print(accuracy_score(y_test,rfc_3.predict(df_test)))

"""As we can see removing anymore features results in drop of accuracy by further 1.5% which is not acceptable.

---



---

**Inference:**
1. Using SMOTE gives a better result than using RandomOverSampler.
2. We were able to derive the best features which could be used for the RandomForestClassifier, SVC, LogisticRegression.
3. Among the three models the accuracy of RandomForestClassifier was the best 95%.
4. By reducing the features from 86 to 36 using the correlation values there was a drop of only 1% in the accuracy of the model.

---



---

**Next step is to perform PCA on the remaining data.**
"""

